{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc0fe45d-3227-4f84-8f64-1c4377250b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a086dafb-b8a9-4eb2-95be-d2fb17b147fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataPrep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "import nltk.corpus \n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fedcb642-af99-4096-9d3a-80efa788637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will start with simple bag of words technique \n",
    "#creating feature vector - document term matrix\n",
    "countV = CountVectorizer()\n",
    "train_count = countV.fit_transform(DataPrep.train_news['Statement'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3eb1f44e-34d0-4a61-aadd-50aa4acb8f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n",
      "  (0, 9676)\t1\n",
      "  (0, 10988)\t1\n",
      "  (0, 1044)\t1\n",
      "  (0, 6639)\t1\n",
      "  (0, 8376)\t1\n",
      "  (0, 5115)\t1\n",
      "  (0, 10709)\t1\n",
      "  (0, 11036)\t1\n",
      "  (0, 11296)\t1\n",
      "  (0, 615)\t1\n",
      "  (0, 7728)\t1\n",
      "  (0, 3278)\t1\n",
      "  (1, 10988)\t1\n",
      "  (1, 11934)\t2\n",
      "  (1, 3434)\t1\n",
      "  (1, 3185)\t1\n",
      "  (1, 7672)\t1\n",
      "  (1, 2475)\t1\n",
      "  (1, 10425)\t1\n",
      "  (1, 6052)\t1\n",
      "  (1, 10426)\t2\n",
      "  (1, 7418)\t1\n",
      "  (1, 4860)\t1\n",
      "  (1, 11138)\t1\n",
      "  (1, 7674)\t1\n",
      "  :\t:\n",
      "  (10239, 10988)\t1\n",
      "  (10239, 7672)\t2\n",
      "  (10239, 11110)\t2\n",
      "  (10239, 5267)\t1\n",
      "  (10239, 7828)\t1\n",
      "  (10239, 7824)\t1\n",
      "  (10239, 1159)\t1\n",
      "  (10239, 12151)\t2\n",
      "  (10239, 6327)\t1\n",
      "  (10239, 6603)\t1\n",
      "  (10239, 11013)\t1\n",
      "  (10239, 11004)\t1\n",
      "  (10239, 3309)\t1\n",
      "  (10239, 12158)\t1\n",
      "  (10239, 11660)\t2\n",
      "  (10239, 799)\t1\n",
      "  (10239, 2568)\t1\n",
      "  (10239, 11622)\t1\n",
      "  (10239, 2549)\t1\n",
      "  (10239, 10660)\t1\n",
      "  (10239, 8996)\t1\n",
      "  (10239, 10918)\t1\n",
      "  (10239, 3989)\t1\n",
      "  (10239, 10594)\t1\n",
      "  (10239, 6853)\t1\n"
     ]
    }
   ],
   "source": [
    "print(countV)\n",
    "print(train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "505625a6-0449-4326-bfca-d15a44bff25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print training doc term matrix\n",
    "#we have matrix of size of (10240, 12196) by calling below\n",
    "def get_countVectorizer_stats():\n",
    "    \n",
    "    #vocab size\n",
    "    train_count.shape\n",
    "\n",
    "    #check vocabulary using below command\n",
    "    print(countV.vocabulary_)\n",
    "\n",
    "    #get feature names\n",
    "    print(countV.get_feature_names()[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb92018a-e737-41a2-94d9-4b4f297b2728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tf-df frequency features\n",
    "#tf-idf \n",
    "tfidfV = TfidfTransformer()\n",
    "train_tfidf = tfidfV.fit_transform(train_count)\n",
    "\n",
    "def get_tfidf_stats():\n",
    "    train_tfidf.shape\n",
    "    #get train data feature names \n",
    "    print(train_tfidf.A[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d3d8bdd-4965-41cc-b9a7-1cc83c4f6f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of words - with n-grams\n",
    "#countV_ngram = CountVectorizer(ngram_range=(1,3),stop_words='english')\n",
    "#tfidf_ngram  = TfidfTransformer(use_idf=True,smooth_idf=True)\n",
    "\n",
    "tfidf_ngram = TfidfVectorizer(stop_words='english',ngram_range=(1,4),use_idf=True,smooth_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6dac79e-401a-40d7-92ad-ed85976a685c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Hayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('treebank')\n",
    "#POS Tagging\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f61cc42a-0eb9-4550-832c-3aeebe65e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = int(.75 * len(tagged_sentences))\n",
    "training_sentences = DataPrep.train_news['Statement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8fec178-95fe-4a2b-8836-0a56c85bbc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Says the Annies List political group supports ...\n",
      "1        When did the decline of coal start? It started...\n",
      "2        Hillary Clinton agrees with John McCain \"by vo...\n",
      "3        Health care reform legislation is likely to ma...\n",
      "4        The economic turnaround started at the end of ...\n",
      "                               ...                        \n",
      "10235    There are a larger number of shark attacks in ...\n",
      "10236    Democrats have now become the party of the [At...\n",
      "10237    Says an alternative to Social Security that op...\n",
      "10238    On lifting the U.S. Cuban embargo and allowing...\n",
      "10239    The Department of Veterans Affairs has a manua...\n",
      "Name: Statement, Length: 10240, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69cae0ff-54a0-45f1-b6a3-43e959de29f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training POS tagger based on words\n",
    "def features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cff09650-2b3d-4425-a723-250a97ad195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to strip tags from tagged corpus\t\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "caf8e19a-3901-4cf9-b680-2de07f2a8e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbfed31f-f27c-4974-bc06-f572dda043ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39685cc9-c459-4d9b-9331-6675a54b2c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c3922db-7921-4f87-b619-78de4c3a77e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass TfidfEmbeddingVectorizer(object):\\n    def __init__(self, word2vec):\\n        self.word2vec = word2vec\\n        self.word2weight = None\\n        self.dim = len(word2vec.itervalues().next())\\n\\n    def fit(self, X, y):\\n        tfidf = TfidfVectorizer(analyzer=lambda x: x)\\n        tfidf.fit(X)\\n        # if a word was never seen - it must be at least as infrequent\\n        # as any of the known words - so the default idf is the max of \\n        # known idf's\\n        max_idf = max(tfidf.idf_)\\n        self.word2weight = defaultdict(\\n            lambda: max_idf,\\n            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\\n\\n        return self\\n\\n    def transform(self, X):\\n        return np.array([\\n                np.mean([self.word2vec[w] * self.word2weight[w]\\n                         for w in words if w in self.word2vec] or\\n                        [np.zeros(self.dim)], axis=0)\\n                for words in X\\n            ])\\n\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbcb3f7-33e7-4ed3-9af4-72f3ce21f14c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
